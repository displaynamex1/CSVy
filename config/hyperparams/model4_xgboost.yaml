model_name: model4_xgboost
description: XGBoost for industry-standard accuracy

hyperparameters:
  learning_rate:
    - 0.001
    - 0.01
    - 0.05
    - 0.1
    - 0.3
  
  n_estimators:
    - 100
    - 200
    - 500
    - 1000
    - 2000
  
  max_depth:
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
  
  min_child_weight:
    - 1
    - 3
    - 5
    - 7
  
  gamma:
    - 0
    - 0.1
    - 0.2
    - 0.5
  
  subsample:
    - 0.5
    - 0.6
    - 0.7
    - 0.8
    - 0.9
    - 1.0
  
  colsample_bytree:
    - 0.5
    - 0.6
    - 0.7
    - 0.8
    - 0.9
    - 1.0
  
  colsample_bylevel:
    - 0.5
    - 0.7
    - 1.0
  
  reg_alpha:
    - 0
    - 0.01
    - 0.1
    - 1
  
  reg_lambda:
    - 0
    - 0.1
    - 1
    - 10
  
  scale_pos_weight:
    - 0.5
    - 1.0
    - 2.0
  
  booster:
    - gbtree
    - gblinear
    - dart
  
  tree_method:
    - auto
    - exact
    - approx
    - hist

defaults:
  learning_rate: 0.05
  n_estimators: 500
  max_depth: 6
  min_child_weight: 3
  gamma: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  colsample_bylevel: 1.0
  reg_alpha: 0.1
  reg_lambda: 1.0
  scale_pos_weight: 1.0
  booster: gbtree
  tree_method: hist
  early_stopping_rounds: 50

notes: |
  XGBoost tuning strategy:
  
  1. Start with defaults
  2. Tune max_depth and min_child_weight (control overfitting)
  3. Tune gamma (minimum loss reduction)
  4. Tune subsample and colsample_bytree (prevent overfitting)
  5. Tune regularization (reg_alpha, reg_lambda)
  6. Lower learning_rate and increase n_estimators
  
  For hockey data:
  - Keep max_depth 4-6 (not too deep)
  - Use early_stopping_rounds=50
  - learning_rate=0.01-0.05 works well
  - Use 'hist' tree_method for speed
  
  This is your accuracy champion - tune carefully!
